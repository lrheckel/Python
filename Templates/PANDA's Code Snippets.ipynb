{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "From this link:\n",
    "\n",
    "https://jeffdelaney.me/blog/useful-snippets-in-pandas/\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Importing a CSV File\n",
    "There are a ton of options for the read_csv function that can simplify preprocessing of data. Nobody want to waste time cleaning data, so see if you can knock it out when import the initial file.\n",
    "\n",
    "df = pd.read_csv('pizza.csv')\n",
    "Need to parse dates? Just pass in the corresponding column name(s).\n",
    "\n",
    "df = pd.read_csv('pizza.csv', parse_dates=['dates'])\n",
    "Only need a few specific columns?\n",
    "\n",
    "df = pd.read_csv('pizza.csv', usecols=['foo', 'bar'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Exploring Data in a DataFrame\n",
    "The first thing you probably want to do is see what the data looks like. Here a few ways to check out Pandas data.\n",
    "\n",
    "df.head()       # first five rows\n",
    "df.tail()       # last five rows\n",
    "df.sample(5)    # random sample of rows\n",
    "df.shape        # number of rows/columns in a tuple\n",
    "df.describe()   # calculates measures of central tendency\n",
    "df.info()       # memory footprint and datatypes\n",
    "Here’s the head of the pizza DataFrame…\n",
    "\n",
    "order_number\tdate\tsize\ttopping\tprice\tdiscount\tcoupon\n",
    "0\tPZZA0001\t08/21/16\tSmall\tAnchovies\t12.99\t3.5\tYes\n",
    "1\tPZZA0000\t09/26/16\tLarge\tPepperoni\t14.50\t0.0\tNo\n",
    "2\tPZZA0001\t09/27/16\tExtra Large\tBell Pepper\t19.99\t0.0\tNo\n",
    "3\tPZZA0002\t09/28/16\tExtra Large\tOlives\t20.99\t5.0\tYes\n",
    "4\tPZZA0003\t09/29/16\tExtra Large\tPepperoni\t21.99\t0.0\tNo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Adding a New Column to a DataFrame\n",
    "The quick and easy way is to just define a new column on the dataframe. This will give us column with the number 23 on every row. Usually, you will be setting the new column with an array or Series that matches the number of rows in the data.\n",
    "\n",
    "df['new_column'] = 23\n",
    "Need to build a new column based on values from other columns?\n",
    "\n",
    "full_price = (df.price + df.discount)\n",
    "df['original_price'] = full_price\n",
    "Need the column in a certain order? The first argument is the position of the column. This will put the column at the begining of the DataFrame.\n",
    "\n",
    "df.insert(0, 'original_price', full_price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Select a Specific “Cell” Value\n",
    "By cell I mean a single row/column intersection, like those in an Excel spreadsheet. You would expect this to be simple, but the syntax is not very obvious. There are three methods in Pandas that almost do the same thing, .loc, iloc, .ix – adding to the confusion for newcomers.\n",
    "\n",
    "Typically, I use .ix because it allows a mix of integers and strings. Enter the index of the row first, then the column.\n",
    "\n",
    "df.ix[2, 'topping']\n",
    "You can also select the column first with dot notation, then the row index, which looks a little cleaner.\n",
    "\n",
    "df.topping.ix[2]\n",
    "Either method will return the value of the cell.\n",
    "\n",
    ">>> 'Bell Pepper'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5.Filtering DataFrames with Conditional Logic\n",
    "Let’s the we need to analyze orders that have pineapple in the topping column.\n",
    "\n",
    "filtered_data = df[df.topping == 'pineapple']\n",
    "Or that meet a certain price threshold\n",
    "\n",
    "filtered_data = df[df.price > 11.99 ]\n",
    "How about both at the same time? Just add the conditions to tuples and connect them with a bitwise operator.\n",
    "\n",
    "filtered_data = df[(df.price > 11.99) & (df.topping == 'Pineapple')]\n",
    "Now we have all the pizzas with a Pineapple topping priced over 11.99.\n",
    "\n",
    "order_number\tdate\tsize\ttopping\tprice\tdiscount\tcoupon\n",
    "6\tPZZA0006\t10/01/16\tMedium\tPineapple\t17.50\t0.0\tNo\n",
    "9\tPZZA0009\t10/04/16\tMedium\tPineapple\t12.99\t2.0\tYes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Sorting a DataFrame by a Certain Column\n",
    "Pretty self-explanatory, but very useful.\n",
    "\n",
    "df.sort_values('price', axis=0, ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Apply a Function to Every Row in a Column\n",
    "Anonymous lambda functions in Python are useful for these tasks. Let’s say we need to calculate taxes for every row in the DataFrame with a custom function. The pandas apply method allows us to pass a function that will run on every value in a column. In this example, we extract a new taxes feature by running a custom function on the price data.\n",
    "\n",
    "def calculate_taxes(price):\n",
    "    taxes = price * 0.12\n",
    "    return taxes\n",
    "\n",
    "df['taxes'] = df.price.apply(calculate_taxes)\n",
    "order_number\tprice\ttaxes\n",
    "0\tPZZA0000\t12.99\t1.5588\n",
    "1\tPZZA0001\t14.50\t1.7400\n",
    "2\tPZZA0002\t19.99\t2.3988\n",
    "3\tPZZA0003\t20.99\t2.5188\n",
    "4\tPZZA0004\t21.99\t2.6388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Add a New Column with Conditional Logic\n",
    "The where function in numpy is useful when extracting features with conditional logic. Let’s imagine the pizza parlor is only profitable on sales above $15.00. We create a new column based on this insight like so:\n",
    "\n",
    "df['profitable'] = np.where(df['price']>=15.00, True, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Finding the Mean or Standard Deviation of Multiple Columns or Rows\n",
    "If you have a DataFrame with the same type of data in every column, possibly a time series with financial data, you may need to find he mean horizontally.\n",
    "\n",
    "df['mean'] = df.mean(axis=1)\n",
    "or to find the standard deviation vertically\n",
    "\n",
    "df.std(axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Converting a DataFrame to a Numpy Array\n",
    "Converting the the values in a DataFrame to an array is simple\n",
    "\n",
    "df.values\n",
    "If you want to preserve the table presentation\n",
    "\n",
    "df.as_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Combining DataFrames with Concatenation\n",
    "You can concatenate rows or columns together, the only requirement is that the shape is the same on corresponding axis. To concat rows vertically:\n",
    "\n",
    "pd.concat([df_1, df_2], axis=0)\n",
    "Or to concat columns horizontally:\n",
    "\n",
    "pd.concat([df_1, df_2], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Combining DataFrames based on an Index Key\n",
    "Merging in Pandas works just like SQL. If you you have two DataFrames that share a key, perhaps a pizza ‘order_id’, you can perform inner, outer, left, right joins just like you would in SQL.\n",
    "\n",
    "merged_df = df_1.merge(df_2, how='left', on='order_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13. Converting Dates to their own Day, Week, Month, Year Columns\n",
    "First, make sure the data is in datetime format. Then use dt method to extract the data you need.\n",
    "\n",
    "date = pd.to_datetime(df.date)\n",
    "df['weekday'] = date.dt.weekday\n",
    "df['year'] = date.dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14. Finding NaNs in a DataFrame\n",
    "Count the total number of NaNs present:\n",
    "\n",
    "df.isnull().sum().sum()\n",
    "List the NaN count for each column:\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Filling NaNs or Missing Data\n",
    "Most machine learning algorithms do not like NaN values, so you’ll probably need to convert them. If the topping column is missing some values, we can fill them a default value.\n",
    "\n",
    "df.topping = df.topping.fillna('Cheese')\n",
    "or we can drop any row missing data across the entire DataFrame:\n",
    "\n",
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16. Extracting Features by Grouping Columns\n",
    "Grouping columns is a great way to extract features from data. This is especially useful when you have data that can be counted or quantified in some way. For example, you might have group pizzas by topping, then calculate the mean for price in each group.\n",
    "\n",
    "df.groupby('topping')['discount'].apply(lambda x: np.mean(x))\n",
    "or maybe you want to see the count of a certain value\n",
    "\n",
    "df.groupby('topping')['discount'].apply(lambda x: x.count())\n",
    "topping\n",
    "Anchovies      3\n",
    "Bell Pepper    1\n",
    "Cheese         2\n",
    "Olives         1\n",
    "Pepperoni      3\n",
    "Pineapple      2\n",
    "Veggie         1\n",
    "Name: discount, dtype: int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "17.Creating Bins\n",
    "Let’s say we want to create 3 separate bins for different price ranges. This is especially useful for simplifying noisy data.\n",
    "\n",
    "bins = [0, 5, 15, 30]\n",
    "names = ['Cheap', 'Normal', 'Expensive']\n",
    "\n",
    "df['price_point'] = pd.cut(df.price, bins, labels=names)\n",
    "order_number\tprice\tprice_point\n",
    "0\tPZZA0000\t12.99\tNormal\n",
    "1\tPZZA0001\t14.50\tNormal\n",
    "2\tPZZA0002\t19.99\tExpensive\n",
    "3\tPZZA0003\t20.99\tExpensive\n",
    "4\tPZZA0004\t21.99\tExpensive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18. Creating a new Column by Looping\n",
    "Let’s say we want to categorize toppings by ‘vegetable’ or ‘meat’. Dealing with nominal values like these can be handled with a for loop. (Note: you can also use the apply function described earlier to perform this task. )\n",
    "\n",
    "topping_type = []\n",
    "\n",
    "for row in df.topping:\n",
    "    if row in ['pepperoni', 'chicken', 'anchovies']:\n",
    "        topping_type.append('meat')\n",
    "    else:\n",
    "        topping_type.append('vegetable')\n",
    "\n",
    "df['topping_type'] = topping_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19. Loading Massive Datasets in Smaller Chunks\n",
    "Sometimes you might have a massive file that will max out your RAM and crash your system. In that case, you might need to analyze the file in smaller chunks.\n",
    "\n",
    "chunksize = 500\n",
    "chunks = []\n",
    "for chunk in pd.read_csv('pizza.csv', chunksize=chunksize):\n",
    "    # Do stuff...\n",
    "    chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(chunks, axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
